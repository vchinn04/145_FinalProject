{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "from os.path import join\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import scipy\n",
    "\n",
    "################# Load and Save Data ################\n",
    "\n",
    "def load_json(rfdir, rfname):\n",
    "    with codecs.open(join(rfdir, rfname), 'r', encoding='utf-8') as rf:\n",
    "        return json.load(rf)\n",
    "\n",
    "\n",
    "def dump_json(obj, wfpath, wfname, indent=None):\n",
    "    with codecs.open(join(wfpath, wfname), 'w', encoding='utf-8') as wf:\n",
    "        json.dump(obj, wf, ensure_ascii=False, indent=indent)\n",
    "\n",
    "\n",
    "def dump_data(obj, wfpath, wfname):\n",
    "    with open(os.path.join(wfpath, wfname), 'wb') as wf:\n",
    "        pickle.dump(obj, wf)\n",
    "\n",
    "\n",
    "def load_data(rfpath, rfname):\n",
    "    with open(os.path.join(rfpath, rfname), 'rb') as rf:\n",
    "        return pickle.load(rf)\n",
    "\n",
    "################ check & mkdir ###########################\n",
    "\n",
    "def check_mkdir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    " \n",
    "################# Random Walk by Metapath ################\n",
    "\n",
    "import random\n",
    "class MetaPathGenerator:\n",
    "    def __init__(self):\n",
    "        self.paper_author = dict()\n",
    "        self.author_paper = dict()\n",
    "        self.paper_org = dict()\n",
    "        self.org_paper = dict()\n",
    "        self.paper_ven = dict()\n",
    "        self.ven_paper = dict()\n",
    "        self.paper_word = dict()\n",
    "        self.word_paper = dict()\n",
    "\n",
    "    def read_data(self, dirpath):\n",
    "        temp=set()\n",
    "\n",
    "        with open(dirpath + \"/paper_org.txt\", encoding='utf-8') as pafile:\n",
    "            for line in pafile:\n",
    "                temp.add(line)                       \n",
    "        for line in temp: \n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    p, a = toks[0], toks[1]\n",
    "                    if p not in self.paper_org:\n",
    "                        self.paper_org[p] = []\n",
    "                    self.paper_org[p].append(a)\n",
    "                    if a not in self.org_paper:\n",
    "                        self.org_paper[a] = []\n",
    "                    self.org_paper[a].append(p)\n",
    "        temp.clear()\n",
    "        \n",
    "              \n",
    "        with open(dirpath + \"/paper_author.txt\", encoding='utf-8') as pafile:\n",
    "            for line in pafile:\n",
    "                temp.add(line)                       \n",
    "        for line in temp: \n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    p, a = toks[0], toks[1]\n",
    "                    if p not in self.paper_author:\n",
    "                        self.paper_author[p] = []\n",
    "                    self.paper_author[p].append(a)\n",
    "                    if a not in self.author_paper:\n",
    "                        self.author_paper[a] = []\n",
    "                    self.author_paper[a].append(p)\n",
    "        temp.clear()\n",
    "                \n",
    "        with open(dirpath + \"/paper_venue.txt\", encoding='utf-8') as pcfile:\n",
    "            for line in pcfile:\n",
    "                temp.add(line)                       \n",
    "        for line in temp: \n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    p, a = toks[0], toks[1]\n",
    "                    if p not in self.paper_ven:\n",
    "                        self.paper_ven[p] = []\n",
    "                    self.paper_ven[p].append(a)\n",
    "                    if a not in self.ven_paper:\n",
    "                        self.ven_paper[a] = []\n",
    "                    self.ven_paper[a].append(p)\n",
    "        temp.clear()\n",
    "        \n",
    "        \n",
    "        with open(dirpath + \"/paper_word.txt\", encoding='utf-8') as pcfile:\n",
    "            for line in pcfile:\n",
    "                temp.add(line)                       \n",
    "        for line in temp: \n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    p, a = toks[0], toks[1]\n",
    "                    if p not in self.paper_word:\n",
    "                        self.paper_word[p] = []\n",
    "                    self.paper_word[p].append(a)\n",
    "                    if a not in self.word_paper:\n",
    "                        self.word_paper[a] = []\n",
    "                    self.word_paper[a].append(p)\n",
    "        temp.clear()\n",
    "                    \n",
    "        print (\"#papers \", len(self.paper_ven))      \n",
    "        print (\"#authors\", len(self.author_paper))\n",
    "        print (\"#org_words\", len(self.org_paper))\n",
    "        print (\"#vens  \", len(self.ven_paper)) \n",
    "    \n",
    "    \n",
    "    ## 基于元路径的随机游走 , 存储PaperID的路径 \n",
    "    def generate_WMRW(self, outfilename, numwalks, walklength):   \n",
    "        outfile = open(outfilename, 'w')\n",
    "        for paper0 in self.paper_ven: \n",
    "            for j in range(0, numwalks): \n",
    "                paper=paper0\n",
    "                outline = \"\"\n",
    "                i=0\n",
    "                while(i<walklength):\n",
    "                    i=i+1    \n",
    "                    if paper in self.paper_author:         #按CoAuthor的数量，以等比例概率游走到下一篇paper\n",
    "                        authors = self.paper_author[paper]\n",
    "                        numa = len(authors)\n",
    "                        authorid = random.randrange(numa)\n",
    "                        author = authors[authorid]\n",
    "                        \n",
    "                        papers = self.author_paper[author]\n",
    "                        nump = len(papers)\n",
    "                        if nump >1:\n",
    "                            paperid = random.randrange(nump)\n",
    "                            paper1 = papers[paperid]\n",
    "                            while paper1 == paper:\n",
    "                                paperid = random.randrange(nump)\n",
    "                                paper1 = papers[paperid]\n",
    "                            paper = paper1\n",
    "                            outline += \" \" + paper           \n",
    "                        \n",
    "                    if paper in self.paper_org:            #按CoOrg的数量，以等比例概率游走到下一篇paper\n",
    "                        words = self.paper_org[paper]\n",
    "                        numw = len(words)\n",
    "                        wordid = random.randrange(numw) \n",
    "                        word = words[wordid]\n",
    "                    \n",
    "                        papers = self.org_paper[word]\n",
    "                        nump = len(papers)\n",
    "                        if nump >1:\n",
    "                            paperid = random.randrange(nump)\n",
    "                            paper1 = papers[paperid]\n",
    "                            while paper1 == paper:\n",
    "                                paperid = random.randrange(nump)\n",
    "                                paper1 = papers[paperid]\n",
    "                            paper = paper1\n",
    "                            outline += \" \" + paper  \n",
    "                    \n",
    "                    r_index = random.random()\n",
    "                    if r_index >= 0.9:                  \n",
    "                        if paper in self.paper_ven:        #按CoVenue游走到下一篇paper   (不一定每次游走都包含)\n",
    "                            words = self.paper_ven[paper]\n",
    "                            numw = len(words)\n",
    "                            wordid = random.randrange(numw) \n",
    "                            word = words[wordid]\n",
    "\n",
    "                            papers = self.ven_paper[word]\n",
    "                            nump = len(papers)\n",
    "                            if nump >1:\n",
    "                                paperid = random.randrange(nump)\n",
    "                                paper1 = papers[paperid]\n",
    "                                while paper1 == paper:\n",
    "                                    paperid = random.randrange(nump)\n",
    "                                    paper1 = papers[paperid]\n",
    "                                paper = paper1\n",
    "                                outline += \" \" + paper\n",
    "                    \n",
    "                    r_index = random.random()\n",
    "                    if r_index >= 0.9:                  \n",
    "                        if paper in self.paper_word:        #按CoWord游走到下一篇paper   (不一定每次游走都包含)\n",
    "                            words = self.paper_word[paper]\n",
    "                            numw = len(words)\n",
    "                            wordid = random.randrange(numw) \n",
    "                            word = words[wordid]\n",
    "\n",
    "                            papers = self.word_paper[word]\n",
    "                            nump = len(papers)\n",
    "                            if nump >1:\n",
    "                                paperid = random.randrange(nump)\n",
    "                                paper1 = papers[paperid]\n",
    "                                while paper1 == paper:\n",
    "                                    paperid = random.randrange(nump)\n",
    "                                    paper1 = papers[paperid]\n",
    "                                paper = paper1\n",
    "                                outline += \" \" + paper\n",
    "                                \n",
    "                outfile.write(outline + \"\\n\")\n",
    "        outfile.close()\n",
    "        \n",
    "        print (\"walks done\")\n",
    "        \n",
    "################# Compare Lists ################\n",
    "\n",
    "def tanimoto(p,q):          ##交集除以并集 \n",
    "    c = [v for v in p if v in q]\n",
    "    return float(len(c) / (len(p) + len(q) - len(c)))\n",
    "\n",
    "\n",
    "################# Paper similarity ################\n",
    "\n",
    "def generate_pair(pubs,outlier): ##求已分配paper和离群点paper的匹配相似度\n",
    "    dirpath = 'gene'\n",
    "    \n",
    "    paper_org = {}\n",
    "    paper_ven = {}\n",
    "    paper_author = {}\n",
    "    paper_word = {}\n",
    "    \n",
    "    temp=set()\n",
    "    with open(dirpath + \"/paper_org.txt\", encoding='utf-8') as pafile:\n",
    "        for line in pafile:\n",
    "            temp.add(line)                       \n",
    "    for line in temp: \n",
    "        toks = line.strip().split(\"\\t\")\n",
    "        if len(toks) == 2:\n",
    "            p, a = toks[0], toks[1]\n",
    "            if p not in paper_org:\n",
    "                paper_org[p] = []\n",
    "            paper_org[p].append(a)\n",
    "    temp.clear()\n",
    "    \n",
    "    with open(dirpath + \"/paper_ven.txt\", encoding='utf-8') as pafile:\n",
    "        for line in pafile:\n",
    "            temp.add(line)                       \n",
    "    for line in temp: \n",
    "        toks = line.strip().split(\"\\t\")\n",
    "        if len(toks) == 2:\n",
    "            p, a = toks[0], toks[1]\n",
    "            if p not in paper_ven:\n",
    "                paper_ven[p]=[]\n",
    "            paper_ven[p]=a\n",
    "    temp.clear()\n",
    "    \n",
    "    with open(dirpath + \"/paper_author.txt\", encoding='utf-8') as pafile:\n",
    "        for line in pafile:\n",
    "            temp.add(line)                       \n",
    "    for line in temp: \n",
    "        toks = line.strip().split(\"\\t\")\n",
    "        if len(toks) == 2:\n",
    "            p, a = toks[0], toks[1]\n",
    "            if p not in paper_author:\n",
    "                paper_author[p] = []\n",
    "            paper_author[p].append(a)\n",
    "    temp.clear()\n",
    "       \n",
    "    with open(dirpath + \"/paper_word.txt\", encoding='utf-8') as pafile:\n",
    "        for line in pafile:\n",
    "            temp.add(line)                       \n",
    "    for line in temp: \n",
    "        toks = line.strip().split(\"\\t\")\n",
    "        if len(toks) == 2:\n",
    "            p, a = toks[0], toks[1]\n",
    "            if p not in paper_word:\n",
    "                paper_word[p] = []\n",
    "            paper_word[p].append(a)\n",
    "    temp.clear()\n",
    "    \n",
    "    \n",
    "    paper_paper = np.zeros((len(pubs),len(pubs)))           \n",
    "    for i,pid in enumerate(pubs):\n",
    "        if i not in outlier:\n",
    "            continue\n",
    "        for j,pjd in enumerate(pubs):\n",
    "            if j==i:\n",
    "                continue\n",
    "            ca=0\n",
    "            cv=0\n",
    "            co=0\n",
    "            ct=0\n",
    "          \n",
    "            if pid in paper_author and pjd in paper_author:\n",
    "                ca = len(set(paper_author[pid])&set(paper_author[pjd]))*1.75     ##若两篇文章都在p_a中，取交集*1.75\n",
    "            if pid in paper_ven and pjd in paper_ven and 'null' not in paper_ven[pid]:\n",
    "                cv = tanimoto(set(paper_ven[pid]),set(paper_ven[pjd]))           ##若两篇文章都在p_v中，取tanimoto指数\n",
    "            if pid in paper_org and pjd in paper_org:\n",
    "                co = tanimoto(set(paper_org[pid]),set(paper_org[pjd]))          ##若两篇文章都在p_o中，取tanimoto指数\n",
    "            if pid in paper_word and pjd in paper_word:\n",
    "                ct = len(set(paper_word[pid])&set(paper_word[pjd]))/2           ##若两篇文章都在p_w中，取交集/2\n",
    "                    \n",
    "            paper_paper[i][j] =ca+cv+co+ct                                      ##相似度取4个值相加\n",
    "            \n",
    "    return paper_paper\n",
    "\n",
    "    \n",
    "        \n",
    "################# Evaluation Metric -- -- F1 score ################\n",
    "        \n",
    "def pairwise_evaluate(correct_labels,pred_labels):\n",
    "    TP = 0.0  # Pairs Correctly Predicted To SameAuthor\n",
    "    TP_FP = 0.0  # Total Pairs Predicted To SameAuthor\n",
    "    TP_FN = 0.0  # Total Pairs To SameAuthor\n",
    "\n",
    "    for i in range(len(correct_labels)):\n",
    "        for j in range(i + 1, len(correct_labels)):\n",
    "            if correct_labels[i] == correct_labels[j]:\n",
    "                TP_FN += 1\n",
    "            if pred_labels[i] == pred_labels[j]:\n",
    "                TP_FP += 1\n",
    "            if (correct_labels[i] == correct_labels[j]) and (pred_labels[i] == pred_labels[j]):\n",
    "                TP += 1\n",
    "\n",
    "    if TP == 0:\n",
    "        pairwise_precision = 0\n",
    "        pairwise_recall = 0\n",
    "        pairwise_f1 = 0\n",
    "    else:\n",
    "        pairwise_precision = TP / TP_FP\n",
    "        pairwise_recall = TP / TP_FN\n",
    "        pairwise_f1 = (2 * pairwise_precision * pairwise_recall) / (pairwise_precision + pairwise_recall)\n",
    "    return pairwise_precision, pairwise_recall, pairwise_f1\n",
    "\n",
    "\n",
    "################# Save Paper Features ################\n",
    "\n",
    "def save_relation(name_pubs_raw, name): # 保存论文的各种feature\n",
    "    name_pubs_raw = load_json('genename', name_pubs_raw)\n",
    "    save_model_name = \"word2vec/Aword2vec.model\"\n",
    "    model_w = word2vec.Word2Vec.load(save_model_name)\n",
    "    \n",
    "    r = '[!“”\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~—～’]+'\n",
    "    stopword = ['at','based','in','of','for','on','and','to','an','using','with','the','by','we','be','is','are','can']   #通用停用词\n",
    "    stopword1 = ['university','univ','china','department','dept','laboratory','lab','school','al','et',\n",
    "                 'institute','inst','college','chinese','beijing','journal','science','international']                    ##特别停用词\n",
    "    \n",
    "    f1 = open ('gene/paper_author.txt','w',encoding = 'utf-8')\n",
    "    f2 = open ('gene/paper_ven.txt','w',encoding = 'utf-8')\n",
    "    f3 = open ('gene/paper_word.txt','w',encoding = 'utf-8')\n",
    "    f4 = open ('gene/paper_org.txt','w',encoding = 'utf-8')\n",
    "    f5 = open ('gene/paper_venue.txt','w',encoding = 'utf-8')\n",
    "\n",
    "    \n",
    "    taken = name.split(\"_\")\n",
    "    name = taken[0] + taken[1]\n",
    "    name_reverse = taken[1]  + taken[0]\n",
    "    if len(taken)>2:\n",
    "        name = taken[0] + taken[1] + taken[2]\n",
    "        name_reverse = taken[2]  + taken[0] + taken[1]\n",
    "    \n",
    "    authorname_dict={}\n",
    "    ptext_emb = {}  \n",
    "    \n",
    "    tcp=set()  \n",
    "    for i,pid in enumerate(name_pubs_raw):\n",
    "        \n",
    "        pub = name_pubs_raw[pid]\n",
    "        \n",
    "        #save authors\n",
    "        org=\"\"\n",
    "        for author in pub[\"authors\"]:\n",
    "            authorname = re.sub(r,'', author[\"name\"]).lower()      \n",
    "            taken = authorname.split(\" \")\n",
    "            if len(taken)==2:                                   \n",
    "                authorname = taken[0] + taken[1]\n",
    "                authorname_reverse = taken[1]  + taken[0] \n",
    "            \n",
    "                if authorname not in authorname_dict:\n",
    "                    if authorname_reverse not in authorname_dict:\n",
    "                        authorname_dict[authorname]=1\n",
    "                    else:\n",
    "                        authorname = authorname_reverse \n",
    "            else:\n",
    "                authorname = authorname.replace(\" \",\"\")\n",
    "            \n",
    "            if authorname!=name and authorname!=name_reverse:\n",
    "                f1.write(pid + '\\t' + authorname + '\\n')\n",
    "        \n",
    "            else:\n",
    "                if \"org\" in author:\n",
    "                    org = author[\"org\"]\n",
    "                    \n",
    "                    \n",
    "        #save org 待消歧作者的机构名\n",
    "        pstr = org.strip()\n",
    "        pstr = pstr.lower() #小写\n",
    "        pstr = re.sub(r,' ', pstr) #去除符号\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip() #去除多余空格\n",
    "        pstr = pstr.split(' ')\n",
    "        pstr = [word for word in pstr if len(word)>1]\n",
    "        pstr = [word for word in pstr if word not in stopword1]\n",
    "        pstr = [word for word in pstr if word not in stopword]\n",
    "        pstr=set(pstr)\n",
    "        for word in pstr:\n",
    "            f4.write(pid + '\\t' + word + '\\n')\n",
    "            \n",
    "            \n",
    "        #save venue\n",
    "        if \"venue\" in pub and type(pub[\"venue\"]) is str:\n",
    "            venue = pub[\"venue\"]\n",
    "            f5.write(pid + '\\t' + venue + '\\n')\n",
    "            pstr = pub[\"venue\"].strip()\n",
    "            pstr = pstr.lower()\n",
    "            pstr = re.sub(r,' ', pstr)\n",
    "            pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "            pstr = pstr.split(' ')\n",
    "            pstr = [word for word in pstr if len(word)>1]\n",
    "            pstr = [word for word in pstr if word not in stopword1]\n",
    "            pstr = [word for word in pstr if word not in stopword]\n",
    "            if len(pstr)==0:\n",
    "                venue = 'null'\n",
    "                f2.write(pid + '\\t' + venue + '\\n')\n",
    "            else:\n",
    "                for word in pstr:\n",
    "                    f2.write(pid + '\\t' + word + '\\n')\n",
    "                venue = ' '.join(pstr)  \n",
    "        else:\n",
    "            venue = 'null'\n",
    "            f2.write(pid + '\\t' + venue + '\\n')\n",
    "            f5.write(pid + '\\t' + venue + '\\n')\n",
    "\n",
    "            \n",
    "        #save text\n",
    "        pstr = \"\"    \n",
    "        keyword=\"\"\n",
    "        if \"keywords\" in pub:\n",
    "            for word in pub[\"keywords\"]:\n",
    "                keyword=keyword+word+\" \"\n",
    "        pstr = pstr + pub[\"title\"]\n",
    "        pstr=pstr.strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        pstr = pstr.split(' ')\n",
    "        pstr = [word for word in pstr if len(word)>1]\n",
    "        pstr = [word for word in pstr if word not in stopword]\n",
    "        for word in pstr:\n",
    "            f3.write(pid + '\\t' + word + '\\n')\n",
    "        \n",
    "        #save all words' embedding\n",
    "\n",
    "        pstr = keyword + \" \" + pub[\"title\"] + \" \" + venue + \" \" + org\n",
    "        if \"year\" in pub:\n",
    "              pstr = pstr +  \" \" + str(pub[\"year\"])\n",
    "        pstr=pstr.strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        pstr = pstr.split(' ')\n",
    "        pstr = [word for word in pstr if len(word)>2]\n",
    "        pstr = [word for word in pstr if word not in stopword]\n",
    "        pstr = [word for word in pstr if word not in stopword1]\n",
    "\n",
    "        words_vec=[]\n",
    "        for word in pstr:\n",
    "            if (word in model_w):\n",
    "                words_vec.append(model_w[word])\n",
    "        if len(words_vec)<1:\n",
    "            words_vec.append(np.zeros(100))\n",
    "            tcp.add(i)\n",
    "            #print ('outlier:',pid,pstr)\n",
    "        ptext_emb[pid] = np.mean(words_vec,0)\n",
    "        \n",
    "    #  ptext_emb: key is paper id, and the value is the paper's text embedding\n",
    "    dump_data(ptext_emb,'gene','ptext_emb.pkl')\n",
    "    # the paper index that lack text information\n",
    "    dump_data(tcp,'gene','tcp.pkl')\n",
    "            \n",
    " \n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    f3.close()\n",
    "    f4.close()\n",
    "    f5.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 改成你自己的数据集路径\n",
    "base = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 将所有论文的文本信息输入w2v中，包括：organizition title abstract venue year\n",
    "\n",
    "import codecs\n",
    "import json\n",
    "from os.path import join\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "pubs_raw = load_json(base, \"train/train_pub.json\")\n",
    "pubs_raw1 = load_json(base, \"sna_valid/sna_valid_pub.json\")\n",
    "pubs_raw2 = load_json(base, \"sna_test/sna_test_pub.json\")\n",
    "\n",
    "\n",
    "r = '[!“”\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~—～’]+'\n",
    "check_mkdir('./gene')\n",
    "f1 = open ('gene/all_text.txt','w',encoding = 'utf-8')\n",
    "\n",
    "for i,pid in enumerate(pubs_raw):\n",
    "    pub = pubs_raw[pid]\n",
    "    \n",
    "    for author in pub[\"authors\"]:\n",
    "        if \"org\" in author:\n",
    "                org = author[\"org\"]\n",
    "                pstr = org.strip()\n",
    "                pstr = pstr.lower()\n",
    "                pstr = re.sub(r,' ', pstr)\n",
    "                pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "                f1.write(pstr+'\\n')\n",
    "            \n",
    "    title = pub[\"title\"]\n",
    "    pstr=title.strip()\n",
    "    pstr = pstr.lower()\n",
    "    pstr = re.sub(r,' ', pstr)\n",
    "    pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "    f1.write(pstr+'\\n')\n",
    "    \n",
    "    if \"abstract\" in pub and type(pub[\"abstract\"]) is str:\n",
    "        abstract = pub[\"abstract\"]\n",
    "        pstr=abstract.strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        f1.write(pstr+'\\n')\n",
    "        \n",
    "    if \"venue\" in pub and type(pub[\"venue\"]) is str:\n",
    "        venue = pub[\"venue\"]\n",
    "        pstr=venue.strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        f1.write(pstr+'\\n')\n",
    "    \n",
    "    if \"year\" in pub and type(pub[\"year\"]) is int:\n",
    "        pstr = str(pub[\"year\"])\n",
    "        f1.write(pstr+'\\n')\n",
    "    \n",
    "for i,pid in enumerate(pubs_raw1):\n",
    "    pub = pubs_raw1[pid]\n",
    "    \n",
    "    for author in pub[\"authors\"]:\n",
    "        if \"org\" in author:\n",
    "                org = author[\"org\"]\n",
    "                pstr = org.strip()\n",
    "                pstr = pstr.lower()\n",
    "                pstr = re.sub(r,' ', pstr)\n",
    "                pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "                f1.write(pstr+'\\n')\n",
    "            \n",
    "    title = pub[\"title\"]\n",
    "    pstr=title.strip()\n",
    "    pstr = pstr.lower()\n",
    "    pstr = re.sub(r,' ', pstr)\n",
    "    pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "    f1.write(pstr+'\\n')\n",
    "    \n",
    "    if \"abstract\" in pub and type(pub[\"abstract\"]) is str:\n",
    "        abstract = pub[\"abstract\"]\n",
    "        pstr=abstract.strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        f1.write(pstr+'\\n')\n",
    "        \n",
    "    if \"venue\" in pub and type(pub[\"venue\"]) is str:\n",
    "        venue = pub[\"venue\"]\n",
    "        pstr=venue.strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        f1.write(pstr+'\\n')\n",
    "        \n",
    "    if \"year\" in pub and type(pub[\"year\"]) is int:\n",
    "        pstr = str(pub[\"year\"])\n",
    "        f1.write(pstr+'\\n')\n",
    "\n",
    "\n",
    "for i,pid in enumerate(pubs_raw2):\n",
    "    pub = pubs_raw2[pid]\n",
    "    \n",
    "    for author in pub[\"authors\"]:\n",
    "        if \"org\" in author:\n",
    "                org = author[\"org\"]\n",
    "                pstr = org.strip()\n",
    "                pstr = pstr.lower()\n",
    "                pstr = re.sub(r,' ', pstr)\n",
    "                pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "                f1.write(pstr+'\\n')\n",
    "            \n",
    "    title = pub[\"title\"]\n",
    "    pstr=title.strip()\n",
    "    pstr = pstr.lower()\n",
    "    pstr = re.sub(r,' ', pstr)\n",
    "    pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "    f1.write(pstr+'\\n')\n",
    "    \n",
    "    if \"abstract\" in pub and type(pub[\"abstract\"]) is str:\n",
    "        abstract = pub[\"abstract\"]\n",
    "        pstr=abstract.strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        f1.write(pstr+'\\n')\n",
    "        \n",
    "    if \"venue\" in pub and type(pub[\"venue\"]) is str:\n",
    "        venue = pub[\"venue\"]\n",
    "        pstr=venue.strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        f1.write(pstr+'\\n')\n",
    "        \n",
    "    if \"year\" in pub and type(pub[\"year\"]) is int:\n",
    "        pstr = str(pub[\"year\"])\n",
    "        f1.write(pstr+'\\n')\n",
    "\n",
    "         \n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "sentences = word2vec.Text8Corpus(r'gene/all_text.txt')\n",
    "check_mkdir('./word2vec')\n",
    "model = word2vec.Word2Vec(sentences, size=200,negative =5, min_count=2, window=5)\n",
    "model.save('word2vec/Aword2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# name disambiguation (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import word2vec\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "pubs_raw = load_json(base,\"sna_test/sna_test_pub.json\")\n",
    "name_raw = load_json(base,\"sna_test/sna_test_raw.json\")\n",
    "\n",
    "result={}\n",
    "\n",
    "for n,name in enumerate(name_raw):\n",
    "    pubs=[]\n",
    "    for cluster in name_raw[name]:\n",
    "        pubs.append(cluster)\n",
    "    \n",
    "    \n",
    "    print (n,name,len(pubs))\n",
    "    if len(pubs)==0:\n",
    "        result[name]=[]\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    ##保存关系\n",
    "    ###############################################################\n",
    "    name_pubs_raw = {}\n",
    "    for i,pid in enumerate(pubs):\n",
    "        name_pubs_raw[pid] = pubs_raw[pid]\n",
    "\n",
    "    check_mkdir('./genename')    \n",
    "    dump_json(name_pubs_raw, 'genename', name+'.json', indent=4)\n",
    "    save_relation(name+'.json', name)  \n",
    "    ###############################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##元路径游走类\n",
    "    ###############################################################r\n",
    "    mpg = MetaPathGenerator()\n",
    "    mpg.read_data(\"gene\")\n",
    "    ###############################################################\n",
    "    \n",
    "\n",
    "    \n",
    "    ##基于论文结构特征的表征向量\n",
    "    ############################################################### \n",
    "    all_embs=[]\n",
    "    rw_num = 10\n",
    "    cp=set()\n",
    "    for k in range(rw_num):\n",
    "        mpg.generate_WMRW(\"gene/RW.txt\",5,20)\n",
    "        sentences = word2vec.Text8Corpus(r'gene/RW.txt')\n",
    "        model = word2vec.Word2Vec(sentences, size=100,negative =25, min_count=1, window=10)\n",
    "        embs=[]\n",
    "        for i,pid in enumerate(pubs):\n",
    "            if pid in model:\n",
    "                embs.append(model[pid])\n",
    "            else:\n",
    "                cp.add(i)\n",
    "                embs.append(np.zeros(100))\n",
    "        all_embs.append(embs)\n",
    "    all_embs= np.array(all_embs)\n",
    "    print ('relational outlier:',cp)    \n",
    "    ############################################################### \n",
    " \n",
    "\n",
    "\n",
    "    ##基于论文文本特征的表征向量\n",
    "    ###############################################################  \n",
    "    ptext_emb=load_data('gene','ptext_emb.pkl')\n",
    "    tcp=load_data('gene','tcp.pkl')\n",
    "    print ('semantic outlier:',tcp)\n",
    "    tembs=[]\n",
    "    for i,pid in enumerate(pubs):\n",
    "        tembs.append(ptext_emb[pid])\n",
    "    ###############################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##论文相似性矩阵\n",
    "    ###############################################################\n",
    "    sk_sim = np.zeros((len(pubs),len(pubs)))\n",
    "    for k in range(rw_num):\n",
    "        sk_sim = sk_sim + pairwise_distances(all_embs[k],metric=\"cosine\")\n",
    "    sk_sim =sk_sim/rw_num    \n",
    "    \n",
    "\n",
    "    tembs = pairwise_distances(tembs,metric=\"cosine\")\n",
    "   \n",
    "    w = 1   ##加权权重    结构特征：文本特征 = 1:1\n",
    "    sim = (np.array(sk_sim) + w*np.array(tembs))/(1+w)\n",
    "    ############################################################### \n",
    "    \n",
    "    \n",
    "  \n",
    "    ## 聚类\n",
    "    ###############################################################\n",
    "    pre = DBSCAN(eps = 0.2, min_samples = 4,metric =\"precomputed\").fit_predict(sim)\n",
    "    pre= np.array(pre)\n",
    "    \n",
    "    \n",
    "    ##离群论文集\n",
    "    outlier=set()\n",
    "    for i in range(len(pre)):\n",
    "        if pre[i]==-1:\n",
    "            outlier.add(i)\n",
    "    for i in cp:\n",
    "        outlier.add(i)\n",
    "    for i in tcp:\n",
    "        outlier.add(i)\n",
    "            \n",
    "        \n",
    "    ##基于阈值的相似性匹配，当阈值大于等于1.5时，将离群点论文分配给当前作者，否则视为新作者\n",
    "    paper_pair = generate_pair(pubs,outlier)\n",
    "    paper_pair1 = paper_pair.copy()\n",
    "    K = len(set(pre))\n",
    "    for i in range(len(pre)):\n",
    "        if i not in outlier:\n",
    "            continue\n",
    "        j = np.argmax(paper_pair[i])\n",
    "        while j in outlier:\n",
    "            paper_pair[i][j]=-1\n",
    "            j = np.argmax(paper_pair[i])\n",
    "        if paper_pair[i][j]>=1.5:           \n",
    "            pre[i]=pre[j]\n",
    "        else:\n",
    "            pre[i]=K\n",
    "            K=K+1\n",
    "    \n",
    "    for ii,i in enumerate(outlier):\n",
    "        for jj,j in enumerate(outlier):\n",
    "            if jj<=ii:\n",
    "                continue\n",
    "            else:\n",
    "                if paper_pair1[i][j]>=1.5:\n",
    "                    pre[j]=pre[i]\n",
    "            \n",
    "    \n",
    "\n",
    "    print (pre,len(set(pre)))\n",
    "    \n",
    "    result[name]=[]\n",
    "    for i in set(pre):\n",
    "        oneauthor=[]\n",
    "        for idx,j in enumerate(pre):\n",
    "            if i == j:\n",
    "                oneauthor.append(pubs[idx])\n",
    "        result[name].append(oneauthor)\n",
    "    \n",
    "check_mkdir('./genetest')\n",
    "dump_json(result, \"genetest\", \"result_test.json\",indent =4)\n",
    "\n",
    "print('done', datetime.now()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# name disambiguation (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#更新\n",
    "pubs_raw = load_json(\"../data/train\",\"train_pub.json\")\n",
    "name_pubs = load_json(\"../data/train\",\"train_author.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import word2vec\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "\n",
    "result=[]\n",
    "for n,name in enumerate(name_pubs):\n",
    "    ilabel=0\n",
    "    pubs=[] # all papers\n",
    "    labels=[] # ground truth\n",
    "    \n",
    "    for author in name_pubs[name]:\n",
    "        iauthor_pubs = name_pubs[name][author]\n",
    "        for pub in iauthor_pubs:\n",
    "            pubs.append(pub)\n",
    "            labels.append(ilabel)\n",
    "        ilabel += 1\n",
    "        \n",
    "    print (n,name,len(pubs))\n",
    "    \n",
    "    \n",
    "    if len(pubs)==0:\n",
    "        result.append(0)\n",
    "        continue\n",
    "    \n",
    "    ##保存关系\n",
    "    ###############################################################\n",
    "    name_pubs_raw = {}\n",
    "    for i,pid in enumerate(pubs):\n",
    "        name_pubs_raw[pid] = pubs_raw[pid]\n",
    "        \n",
    "    dump_json(name_pubs_raw, 'genename', name+'.json', indent=4)\n",
    "    save_relation(name+'.json', name)  \n",
    "    ###############################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##元路径游走类\n",
    "    ###############################################################r\n",
    "    mpg = MetaPathGenerator()\n",
    "    mpg.read_data(\"gene\")\n",
    "    ###############################################################\n",
    "    \n",
    "  \n",
    "    \n",
    "    ##论文关系表征向量\n",
    "    ############################################################### \n",
    "    all_embs=[]\n",
    "    rw_num =3\n",
    "    cp=set()\n",
    "    for k in range(rw_num):\n",
    "        mpg.generate_WMRW(\"gene/RW.txt\",5,20) #生成路径集\n",
    "        sentences = word2vec.Text8Corpus(r'gene/RW.txt')\n",
    "        model = word2vec.Word2Vec(sentences, size=100,negative =25, min_count=1, window=10)\n",
    "        embs=[]\n",
    "        for i,pid in enumerate(pubs):\n",
    "            if pid in model:\n",
    "                embs.append(model[pid])\n",
    "            else:\n",
    "                cp.add(i)\n",
    "                embs.append(np.zeros(100))\n",
    "        all_embs.append(embs)\n",
    "    all_embs= np.array(all_embs)\n",
    "    print ('relational outlier:',cp)\n",
    "    ############################################################### \n",
    "\n",
    "    \n",
    "    \n",
    "    ##论文文本表征向量\n",
    "    ###############################################################   \n",
    "    ptext_emb=load_data('gene','ptext_emb.pkl')\n",
    "    tcp=load_data('gene','tcp.pkl')\n",
    "    print ('semantic outlier:',tcp)\n",
    "    tembs=[]\n",
    "    for i,pid in enumerate(pubs):\n",
    "        tembs.append(ptext_emb[pid])\n",
    "    ############################################################### \n",
    "    \n",
    "    ##离散点\n",
    "    outlier=set()\n",
    "    for i in cp:\n",
    "        outlier.add(i)\n",
    "    for i in tcp:\n",
    "        outlier.add(i)\n",
    "    \n",
    "    ##网络嵌入向量相似度\n",
    "    sk_sim = np.zeros((len(pubs),len(pubs)))\n",
    "    for k in range(rw_num):\n",
    "        sk_sim = sk_sim + pairwise_distances(all_embs[k],metric=\"cosine\")\n",
    "    sk_sim =sk_sim/rw_num    \n",
    "    \n",
    "    ##文本相似度\n",
    "    t_sim = pairwise_distances(tembs,metric=\"cosine\")\n",
    "    \n",
    "    w = 1\n",
    "    sim = (np.array(sk_sim) + w*np.array(t_sim))/(1+w)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###############################################################\n",
    "    pre = DBSCAN(eps = 0.2, min_samples = 4,metric =\"precomputed\").fit_predict(sim)\n",
    "    \n",
    "    \n",
    "    for i in range(len(pre)):\n",
    "        if pre[i]==-1:\n",
    "            outlier.add(i)\n",
    "    \n",
    "    ## assign each outlier a label\n",
    "    paper_pair = generate_pair(pubs,outlier)\n",
    "    paper_pair1 = paper_pair.copy()\n",
    "    K = len(set(pre))\n",
    "    for i in range(len(pre)):\n",
    "        if i not in outlier:\n",
    "            continue\n",
    "        j = np.argmax(paper_pair[i])\n",
    "        while j in outlier:\n",
    "            paper_pair[i][j]=-1\n",
    "            j = np.argmax(paper_pair[i])\n",
    "        if paper_pair[i][j]>=1.5:\n",
    "            pre[i]=pre[j]\n",
    "        else:\n",
    "            pre[i]=K\n",
    "            K=K+1\n",
    "    \n",
    "    ## find nodes in outlier is the same label or not\n",
    "    for ii,i in enumerate(outlier):\n",
    "        for jj,j in enumerate(outlier):\n",
    "            if jj<=ii:\n",
    "                continue\n",
    "            else:\n",
    "                if paper_pair1[i][j]>=1.5:\n",
    "                    pre[j]=pre[i]\n",
    "            \n",
    "            \n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    pre = np.array(pre)\n",
    "    print (labels,len(set(labels)))\n",
    "    print (pre,len(set(pre)))\n",
    "    pairwise_precision, pairwise_recall, pairwise_f1 = pairwise_evaluate(labels,pre)\n",
    "    print (pairwise_precision, pairwise_recall, pairwise_f1)\n",
    "    result.append(pairwise_f1)\n",
    "\n",
    "print ('avg_f1:', np.mean(result))\n",
    "    \n",
    "print('done', datetime.now()-start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c4b4b156aa8560666d964095090f6db92040aae170280822d9d71c646020d410"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
